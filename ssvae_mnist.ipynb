{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_y(nn.Module):\n",
    "    # outputs a y given an x.\n",
    "    # the classifier. distribution for y given an input x\n",
    "    # input dim is whatever the input image size is,\n",
    "    # output will be the probabilities a that parameterise y ~ cat(a)\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 400)\n",
    "        self.fc2 = nn.Linear(400, output_size)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = self.fc2(y)\n",
    "        y = self.softplus(y)\n",
    "        y = self.softmax(y, dim=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_z(nn.Module):\n",
    "    # input a x and a y, outputs a z\n",
    "    # input x and y as flattened vector\n",
    "    # inputsize should therefore be len(x) + len(y)\n",
    "    def __init__(self, input_size, output_size):\n",
    "    self.fc1 = nn.Linear(input_size, 400)\n",
    "    self.fc2 = nn.Linear(400, 200)\n",
    "    self.fc31 = nn.Linear(200, output_size)\n",
    "    self.fc32 = nn.Linear(200, output_size)\n",
    "    self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        z = self.fc2(z)\n",
    "        z = self.softplus(z)\n",
    "        z_loc = self.fc31(z)\n",
    "        z_scale = torch.exp(self.fc32(z))\n",
    "        return z_loc, z_scale    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # takes y and z and outputs a x\n",
    "    # input shape is therefore y and z concatenated\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.fc1 = nn.Linear(input_size, 300)\n",
    "        self.fc2 = nn.Linear(300, 500)\n",
    "        self.fc3 = nn.Linear(500, output_size)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.fc1(z)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softplus(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim_y, hidden_dim_z use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.encoder_y = Encoder_y(input_size, output_size)\n",
    "        self.encoder_z = Encoder_z(input_size, output_size)\n",
    "        self.decoder = Decoder(input_size, output_size)\n",
    "        \n",
    "    def model(self, xs, ys=None):\n",
    "        # register this pytorch module and all of its sub-modules with pyro\n",
    "        pyro.module(\"ss_vae\", self)\n",
    "        batch_size = xs.size(0)\n",
    "\n",
    "            # inform Pyro that the variables in the batch of xs, ys are conditionally independent\n",
    "        with pyro.plate(\"data\"):\n",
    "            # sample the handwriting style from the constant prior distribution\n",
    "            prior_loc = xs.new_zeros([batch_size, self.z_dim])\n",
    "            prior_scale = xs.new_ones([batch_size, self.z_dim])\n",
    "            zs = pyro.sample(\"z\", dist.Normal(prior_loc, prior_scale).to_event(1))\n",
    "            # if the label y (which digit to write) is supervised, sample from the\n",
    "            # constant prior, otherwise, observe the value (i.e. score it against the constant prior)\n",
    "            alpha_prior = xs.new_ones([batch_size, self.output_size]) / (1.0 * self.output_size)\n",
    "            # vector of probabilities for each class, i.e. output_size\n",
    "            # its a uniform prior\n",
    "            ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha_prior), obs=ys)\n",
    "            # one of the categories will be sampled, according to the distribution specified by alpha prior    \n",
    "            # finally, score the image (x) using the handwriting style (z) and\n",
    "            # the class label y (which digit to write) against the\n",
    "            # parametrized distribution p(x|y,z) = bernoulli(decoder(y,z))\n",
    "            # where `decoder` is a neural network\n",
    "            loc = self.decoder.forward([zs, ys])\n",
    "            # decoder networks takes a category, and a latent variable and outputs an observation x.\n",
    "                pyro.sample(\"x\", dist.Bernoulli(loc).to_event(1), obs=xs)\n",
    "\n",
    "    def guide(self, xs, ys=None):\n",
    "        with pyro.plate(\"data\"):\n",
    "            # if the class label (the digit) is not supervised, sample\n",
    "            # (and score) the digit with the variational distribution\n",
    "            # q(y|x) = categorical(alpha(x))\n",
    "            if ys is None:\n",
    "                # if there is an unlabbeld datapoint, we take the values for x the observations,\n",
    "                # and we output an alpha which parameterises the classifier.\n",
    "            \n",
    "                alpha = self.encoder_y.forward(xs)\n",
    "                # then we sample a classification using this parameterisation of the classifier.\n",
    "                # the classifier is also like a generative model, where given the latents alpha, we \n",
    "                # output an observation y\n",
    "                # and the latents alpha are given by an encoder\n",
    "                ys = pyro.sample(\"y\", dist.OneHotCategorical(alpha))\n",
    "                # if the labels y is known, then we dont have to sample from the above,\n",
    "                # we just feed the actual y in to the encoder that takes x and y.\n",
    "        \n",
    "                # sample (and score) the latent handwriting-style with the variational\n",
    "                # distribution q(z|x,y) = normal(loc(x,y),scale(x,y))\n",
    "                loc, scale = self.encoder_z.forward([xs, ys])\n",
    "                pyro.sample(\"z\", dist.Normal(loc, scale).to_event(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
